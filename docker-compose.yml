services:
  api:
    build:
      context: .
      dockerfile: api/Dockerfile
    image: loseme-api:latest
    ports:
      - "8000:8000"
    environment:
      - QDRANT_URL=http://qdrant:6333
      - LOSEME_DEVICE_ID=MasterMind
      - LOSEME_CHUNKER=semantic
      - LOSEME_EMBEDDING_MODEL=bge-m3
      - LOSEME_VECTOR_STORAGE=qdrant-hybrid
      - HF_HOME=/root/.cache/huggingface
      - TRANSFORMERS_CACHE=/root/.cache/huggingface
      - LOSEME_USE_CUDA=true
    volumes:
      - .:/app
      - metadata:/var/lib/loseme/metadata
      - hf_cache:/root/.cache/huggingface
    command: uvicorn api.app.main:app --host 0.0.0.0 --port 8000 --log-level info --reload
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]

  qdrant:
    image: qdrant/qdrant
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
  
  qdrant_test:
    image: qdrant/qdrant
    profiles: ["test"]
    ports:
      - "6334:6333"
    volumes:
      - qdrant_data_test:/qdrant/storage

  test:
    image: loseme-api:latest
    profiles: ["test"]
    volumes:
      - .:/app
      - metadata_test:/var/lib/loseme/metadata
      - hf_cache:/root/.cache/huggingface

    working_dir: /app
    environment:
      PYTHONPATH: /app
      QDRANT_URL: http://qdrant_test:6333
      ALLOW_VECTOR_CLEAR: "1"
      LOSEME_DEVICE_ID: "MasterMind"
      LOSEME_CHUNKER: semantic
      LOSEME_EMBEDDING_MODEL: nomic-ai/nomic-embed-text-v1
      HF_HOME: /root/.cache/huggingface
      TRANSFORMERS_CACHE: /root/.cache/huggingface
    depends_on:
      - qdrant_test
      - api
    command: pytest -v -s -o log_cli_level=DEBUG
  
  docs:
    build:
      context: .
      dockerfile: docker/docs.Dockerfile
    volumes:
      - .:/app  # Mount local files for live rebuilds
    command: sh -c "make -C docs clean html"

volumes:
  qdrant_data:
  metadata:
  qdrant_data_test:
  metadata_test:
  hf_cache:

